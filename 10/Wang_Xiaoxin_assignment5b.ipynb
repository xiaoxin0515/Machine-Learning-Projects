{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IS590DT2_A5b.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "VXyRlxBCp4lF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and extract the CIFAR10 dataset\n",
        "More info here: https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "metadata": {
        "id": "kNeKsnixgPKf",
        "colab_type": "code",
        "outputId": "bf03629f-bab5-4115-9488-df4c1f93d61b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-30 15:05:03--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  4.46MB/s    in 50s     \n",
            "\n",
            "2018-11-30 15:05:53 (3.26 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sy5-LwWiiFTP",
        "colab_type": "code",
        "outputId": "702f7b26-5f7f-48c1-f1df-cf79f5861653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "!echo Extracting data\n",
        "!tar xf cifar-10-python.tar.gz\n",
        "!echo Done extracting"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data\n",
            "Done extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kxCzuTd9qUci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the images and convert to a format for TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "rE1WwLK6ifk_",
        "colab_type": "code",
        "outputId": "80a66ec2-9645-4606-998e-4ffb7c865aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "def cifar_to_2d(images):\n",
        "    images = np.array(images, dtype='float') / 255\n",
        "    images = images.reshape([-1, 3, 32, 32])  # Convert raw data to 2D images with 3 channels\n",
        "    images = images.transpose(0, 2, 3, 1)  # Put the channels dimension last (TensorFlow requirement)\n",
        "    return images\n",
        "\n",
        "\n",
        "train_X = np.zeros(shape=[50000, 32, 32, 3], dtype=float)  # Placeholder for 50k images, 32x32 with 3 channels\n",
        "train_y = np.zeros(shape=[50000, 10], dtype=int)  # 1-hot encoding 10 classes\n",
        "for i in range(1, 6):\n",
        "    print('Loading training set ' + str(i))\n",
        "    with open('cifar-10-batches-py/data_batch_' + str(i), 'rb') as infile:\n",
        "        img_dict = pickle.load(infile, encoding='bytes')\n",
        "        train_X[(i - 1) * 10000:i * 10000, :] = cifar_to_2d(img_dict[b'data'])\n",
        "        train_y[(i - 1) * 10000:i * 10000, :] = to_categorical(img_dict[b'labels'])\n",
        "\n",
        "print('Loading testing set')\n",
        "with open('cifar-10-batches-py/test_batch', 'rb') as infile:\n",
        "    img_dict = pickle.load(infile, encoding='bytes')\n",
        "    test_X = cifar_to_2d(img_dict[b'data'])\n",
        "    test_y = to_categorical(img_dict[b'labels'])\n",
        "\n",
        "\n",
        "# Choose a random 10% of training data to use as the validation set.\n",
        "np.random.seed(11798)\n",
        "val_i = np.random.choice(len(train_X), 5000, replace=False)\n",
        "val_X = train_X[val_i]\n",
        "val_y = train_y[val_i]\n",
        "train_X = train_X[~np.isin(np.arange(len(train_X)), val_i)]\n",
        "train_y = train_y[~np.isin(np.arange(len(train_y)), val_i)]\n",
        "\n",
        "print(val_X.shape)  # Should now be 5000 32x32 images with 3 channels\n",
        "print(val_y.shape)  # Should now be 5000 one-hot labels with 10 classes\n",
        "print(train_X.shape)  # Should now be 45000 32x32 images with 3 channels\n",
        "print(train_y.shape)  # Should now be 45000 one-hot labels with 10 classes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading training set 1\n",
            "Loading training set 2\n",
            "Loading training set 3\n",
            "Loading training set 4\n",
            "Loading training set 5\n",
            "Loading testing set\n",
            "(5000, 32, 32, 3)\n",
            "(5000, 10)\n",
            "(45000, 32, 32, 3)\n",
            "(45000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jqQpGEoBqjc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Design a convolutional model for classifying the images"
      ]
    },
    {
      "metadata": {
        "id": "-PQ4j-7pqhlz",
        "colab_type": "code",
        "outputId": "55b0a9be-4393-4d30-d81f-4ec92db24ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import models, layers, optimizers\n",
        "\n",
        "\n",
        "# Here we design a model using the \"functional\" Keras API.\n",
        "m_input = layers.Input(shape=train_X[0].shape)  # Use the shape of the first image as input shape.\n",
        "m = m_input\n",
        "\n",
        "#m = layers.Dropout(rate=0.2)(m) # using dropout here will not improve\n",
        "\n",
        "\n",
        "m = layers.Conv2D(filters=100,kernel_size=(2, 2))(m)  # https://keras.io/layers/convolutional/#conv2d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#m = layers.convolutional.SeparableConv2D(filters=100,kernel_size=(2,2))(m)  # this method  is not better than above\n",
        "\n",
        "\n",
        "m = layers.Activation('relu')(m)\n",
        "\n",
        "\n",
        "m = layers.advanced_activations.LeakyReLU(alpha=0.35)(m)# this layer helped a litte\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "m = layers.MaxPooling2D(pool_size=(2, 2))(m)  # https://keras.io/layers/pooling/\n",
        "\n",
        "\n",
        "\n",
        "m = layers.Flatten()(m) \n",
        "\n",
        "\n",
        "\n",
        "m = layers.Dense(50, activation='relu')(m)# used to be 60: 92.96%  After several tests, I found out that 50 is the best with result of 93.14%\n",
        "\n",
        "\n",
        "m = layers.Dropout(rate=0.1)(m)   #  92.8%->93.14%: using dropout here increase 0.3%\n",
        "\n",
        "\n",
        "\n",
        "m = layers.Dense(10, activation='softmax')(m) # activation used to be 'relu'. I found out \"softmax\" is better. \"Softmax\" is also better than \"sigmoid\"  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "m_output = m\n",
        "\n",
        "model = models.Model(m_input, m_output)\n",
        "\n",
        "#opt = optimizers.RMSprop(lr=.001)  # 34.24\n",
        "#opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06) # 0.001 is the best lr.  and accuracy rate is 93.14%\n",
        "#opt = optimizers.Adagrad(lr=.001)  # 91.27%\n",
        "#opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-06) # 93.18%  √\n",
        "opt = optimizers.Adamax(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # 0.003: 93.29%\n",
        "#opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004) # 93.07\n",
        "\n",
        "# I tested out all the optimizers above and found out Adamax is the best with lr at 0.002.\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  #   loss used to be \"categorical_crossentropy\" and I changed it to \"binary_crossentropy\" this increased the accuracy rate a lot.\n",
        "\n",
        "# The accuracy rate in the end is 93.12%\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 31, 31, 100)       1300      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 31, 31, 100)       0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 31, 31, 100)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 100)       0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 22500)             0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 50)                1125050   \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 1,126,860\n",
            "Trainable params: 1,126,860\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6bakQPFOux-g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and validate the model\n",
        "All your hyperparameter tuning needs to be done here!"
      ]
    },
    {
      "metadata": {
        "id": "isU-22Rnu0NE",
        "colab_type": "code",
        "outputId": "bdc2201a-d801-4ac7-dc41-14c5a09d0d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_X, train_y, batch_size=128, epochs=10, validation_data=(val_X, val_y))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "45000/45000 [==============================] - 12s 268us/step - loss: 0.2661 - acc: 0.9042 - val_loss: 0.2268 - val_acc: 0.9132\n",
            "Epoch 2/10\n",
            "45000/45000 [==============================] - 11s 249us/step - loss: 0.2219 - acc: 0.9149 - val_loss: 0.2054 - val_acc: 0.9203\n",
            "Epoch 3/10\n",
            "45000/45000 [==============================] - 11s 247us/step - loss: 0.2047 - acc: 0.9207 - val_loss: 0.1954 - val_acc: 0.9244\n",
            "Epoch 4/10\n",
            "45000/45000 [==============================] - 11s 248us/step - loss: 0.1940 - acc: 0.9245 - val_loss: 0.1891 - val_acc: 0.9273\n",
            "Epoch 5/10\n",
            "45000/45000 [==============================] - 11s 248us/step - loss: 0.1851 - acc: 0.9280 - val_loss: 0.1802 - val_acc: 0.9305\n",
            "Epoch 6/10\n",
            "45000/45000 [==============================] - 11s 247us/step - loss: 0.1784 - acc: 0.9306 - val_loss: 0.1779 - val_acc: 0.9308\n",
            "Epoch 7/10\n",
            "45000/45000 [==============================] - 11s 248us/step - loss: 0.1729 - acc: 0.9327 - val_loss: 0.1751 - val_acc: 0.9319\n",
            "Epoch 8/10\n",
            "45000/45000 [==============================] - 11s 249us/step - loss: 0.1676 - acc: 0.9346 - val_loss: 0.1714 - val_acc: 0.9332\n",
            "Epoch 9/10\n",
            "45000/45000 [==============================] - 11s 248us/step - loss: 0.1618 - acc: 0.9370 - val_loss: 0.1699 - val_acc: 0.9343\n",
            "Epoch 10/10\n",
            "45000/45000 [==============================] - 11s 249us/step - loss: 0.1582 - acc: 0.9383 - val_loss: 0.1723 - val_acc: 0.9334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f941fd44f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "KCtnpomPxE9j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the FINAL model on testing data\n",
        "Only run once, when you are finished adjusting your model"
      ]
    },
    {
      "metadata": {
        "id": "ZTs4PEh5xQW8",
        "colab_type": "code",
        "outputId": "94905785-6ad6-406b-f534-3ca56624ce13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_X, test_y)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 160us/step\n",
            "Test accuracy: 0.9329599878311158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WCJGO5nj0vEn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exporting your results to PDF\n",
        "NOTE: This currently only seems to work in the Chrome browser\n",
        "\n",
        "1. Download your notebook with _File -> Download .ipynb_\n",
        "1. Rename with your name like in other assignments, for example bosch_nigel_assignment5b.ipynb\n",
        "1. Upload to the _Files_ pane on the left side of the screen\n",
        "1. Run the command in the next cell (with your filename) to convert to PDF\n",
        "1. Click _Refresh_ in the Files pane on the left\n",
        "1. Double-click the PDF to download it"
      ]
    },
    {
      "metadata": {
        "id": "BxX0WTJuzaok",
        "colab_type": "code",
        "outputId": "a462d928-c4d7-477b-fcfd-58146c9e97a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "cell_type": "code",
      "source": [
        "!apt install pandoc\n",
        "!apt install texlive-xetex\n",
        "!jupyter nbconvert --to pdf Wang_Xiaoxin_assignment5b.ipynb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "pandoc is already the newest version (1.19.2.4~dfsg-1build4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "texlive-xetex is already the newest version (2017.20180305-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "[NbConvertApp] Converting notebook Wang_Xiaoxin_assignment5b.ipynb to pdf\n",
            "[NbConvertApp] Writing 101832 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex']\n",
            "[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 67491 bytes to Wang_Xiaoxin_assignment5b.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}